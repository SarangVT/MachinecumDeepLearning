{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multi-Layer Perceptron: It actually acts like an Universal Function Approximator.\\nIt handles the problem into parts and stepwise, and then serially add, the results of each layer, again by weights and biases and then it continues, till the output comes.\\nNow, description of the neural network is:\\n1. Input Layer\\n2. Hidden Layer\\n3. Output Layer\\nThey are composed of perceptrons.\\nConnected by Weights and consisted of Biases.\\nIf 1st layer has 4 nodes and 2nd layer has 3 nodes, total unknowns are 4*3=12, weights and 3 biases, total=15\\nNow, bias is named as, bij, i refers to layer number, and j refers to node number in the layer.\\nInput is termed as xij, j refers to node no, i is general notation.\\nOutput is termed as, Oij, where it is same as bias nomenclature, i->layer from which it emits, j->node no\\nWeight is named as, W<sup>k</sup><sub>ij</sub> k refers to layer in which output from it terminates, destination layer.\\ni refers to node no of source layer, j refers to node no of destination layer.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Multi-Layer Perceptron: It actually acts like an Universal Function Approximator.\n",
    "It handles the problem into parts and stepwise, and then serially add, the results of each layer, again by weights and biases and then it continues, till the output comes.\n",
    "Now, description of the neural network is:\n",
    "1. Input Layer\n",
    "2. Hidden Layer\n",
    "3. Output Layer\n",
    "They are composed of perceptrons.\n",
    "Connected by Weights and consisted of Biases.\n",
    "If 1st layer has 4 nodes and 2nd layer has 3 nodes, total unknowns are 4*3=12, weights and 3 biases, total=15\n",
    "Now, bias is named as, bij, i refers to layer number, and j refers to node number in the layer.\n",
    "Input is termed as xij, j refers to node no, i is general notation.\n",
    "Output is termed as, Oij, where it is same as bias nomenclature, i->layer from which it emits, j->node no\n",
    "Weight is named as, W<sup>k</sup><sub>ij</sub> k refers to layer in which output from it terminates, destination layer.\n",
    "i refers to node no of source layer, j refers to node no of destination layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forward Propagation:\\nW<sup>T</sup>X + B\\nE.g. :|W111  W112  W113|T  |Xi1|      |b11| = |O11|\\n      |W121  W122  W123|   |Xi2|   +  |b12| = |O12|\\n      |W131  W132  W133|   |Xi3|      |b13| = |O13|\\nIt gives output of a neural network with 3 input nodes and 1st layer with 3 nodes only.\\nObserve, all weights intiating from a node in source layer, is written in a single row, and then transpose is taken for calculation of weights terminating to the node in output layer with corresponding bias.\\nThen, again this O11, O12, O13 as outputs are again used as inputs for the following layer.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Forward Propagation:\n",
    "W<sup>T</sup>X + B\n",
    "E.g. :|W111  W112  W113|T  |Xi1|      |b11| = |O11|\n",
    "      |W121  W122  W123|   |Xi2|   +  |b12| = |O12|\n",
    "      |W131  W132  W133|   |Xi3|      |b13| = |O13|\n",
    "It gives output of a neural network with 3 input nodes and 1st layer with 3 nodes only.\n",
    "Observe, all weights intiating from a node in source layer, is written in a single row, and then transpose is taken for calculation of weights terminating to the node in output layer with corresponding bias.\n",
    "Then, again this O11, O12, O13 as outputs are again used as inputs for the following layer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loss Functions:\n",
    "1. Regression: MSE, MAE, Huber Loss\n",
    "2. Classification: Binary Cross Entropy, Categorical Cross Entropy, Hinge Loss\n",
    "3. AutoEncoders: KL Divergence\n",
    "4. GAN: Discriminant loss, minmax gain loss\n",
    "5. Object Detection: Focal loss\n",
    "6. Embedding: Triplet Loss\n",
    "\n",
    "Loss Function vs Cost Function:\n",
    "Loss Function is of each data entry while Cost Function is average of all dataset.\n",
    "\n",
    "For MSE as loss, and linear regression type problem, linear must be activation function.\n",
    "Huber Loss: Combination of MSE and MAE\n",
    "L = { 0.5 |y-y'|^2        for |y-y'|<=M\n",
    "      M|y-y'| - (M^2)/2   otherwise\n",
    "    }\n",
    "MSE: No outliers\n",
    "MAE: Few Outliers\n",
    "Huber Loss: Many Outliers\n",
    "\n",
    "Sigmoid as activation function must.\n",
    "Binary Cross Entropy: -ylog(y') - (1-y')log(1-y')\n",
    "y: Actual Value, Target\n",
    "y': NN Prediction(Neural Network)\n",
    "\n",
    "Categorical Cross Entropy: One-hot encoding needed, Softmax, as activation function.\n",
    "-∑(i=1 to n)yjlog(yj')\n",
    "-∑(i=1 to k)∑(i=1 to n)yjlog(yj')\n",
    "n: total number of parameters, k: total number of samples.\n",
    "\n",
    "Sparse Categorical Cross Entropy: One-hot encoding not needed.\n",
    "It is same as, categorical cross entropy, but faster and useful for very large number of classes.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

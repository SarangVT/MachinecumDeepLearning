{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In Deep Learning, generally L2 regularization is used, (ʎ/2n)*∑Wi^2\n",
    "is added to the loss function, caution biases are not added.\n",
    "\n",
    "Just add, a hyperparameter called as,\n",
    "kernel_regularizer = tensorflow.keras.regularizers.l2(0.01)\n",
    "kernel_regularizer = tensorflow.keras.regularizers.l1(0.01)\n",
    "\n",
    "in hidden layers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Activation Functions: Also called as transfer function, is a function applied to the weighted sum of inputs + bias of a perceptron, and the resultant output then according to the layer number and other conditions serves as input for other layers or as final output.\n",
    "Without activation function, it cannot solve non-linear problems, as then g(z) = z\n",
    "Ideal Activation Function:\n",
    "1. Non-linear\n",
    "2. Differentiable (ReLU is not differentiable)\n",
    "3. Computationally inexpensive\n",
    "4. Zero-centered (Standardized data is empirically proven that it gives better result)\n",
    "5. Non-saturated (Squeezes output in a range, e.g.sigmoid, causes vanishing gradient problem)\n",
    "\n",
    "Sigmoid Activation Function: σ(z) = 1/[1+e^(-z)].\n",
    "Pros: 1. As output lies in [0,1] useful in binary classification.\n",
    "2. Non-linear.\n",
    "3. Differentiable σ'(z) = [1-σ(z)][σ(z)]\n",
    "\n",
    "Cons:\n",
    "1. Saturating Problem, should be used in output layer only. Derivative goes to 0, except -6 to 6.\n",
    "2. Non-zero centered output data\n",
    "3. Gradient of all weights in same layer is either positive or negative, this causes restriction and slow convergence.\n",
    "4. Computationally expensive.\n",
    "\n",
    "tanh Activation Function: f(x) = tanh(x) = [e^x - e^(-x)] / [e^x + e^(-x)]\n",
    "f'(x) = 1 - [tanh(x)]^2\n",
    "Pros:\n",
    "1. Non-linear\n",
    "2. Differentiable\n",
    "3. Zero-centered output //Solved of sigmoid\n",
    "\n",
    "Cons:\n",
    "1. Saturating function\n",
    "2. Computationally expensive\n",
    "\n",
    "ReLU Activation Function: max(0,x)\n",
    "Pros:\n",
    "1. Non-linear (Consider as whole not just f(x) = x)\n",
    "2. Non-saturated\n",
    "3. Computationally inexpensive\n",
    "4. Convergence faster\n",
    "\n",
    "Cons:\n",
    "1. Not differentiable, but we consider its derivative 0 for negative and 1 for greater than or equal to zero.\n",
    "2. Output is not zero centered, so we use batch normalization to overcome it.\n",
    "3. Dying ReLU Problem.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dying ReLU Problem:\n",
    "As it may happen that neurons may become dead, and once they are dead they are for forever, they change but negligible.\n",
    "Cause: The input given to ReLU becomes negative thus output zero and derivative of zero is also zero, so within chain rule as one term becomes zero, whole expression shrinks to zero, and no further updation takes place.\n",
    "Reasons:\n",
    "\t1. High learning rate\n",
    "\t2. High -ve bias (Bias intialization, updation)\n",
    "\n",
    "Solution:\n",
    "\t1. Set low learning rate.\n",
    "\t2. Use learning rate = +ve value,usually, 0.01\n",
    "\t3. Don't use ReLU, but its variants,\n",
    "Linear ReLU variants:\n",
    "1. Leaky ReLU\n",
    "2. Parametric ReLU: max\n",
    "\n",
    "Non-Linear ReLU:\n",
    "1. ELU (Exponential Linear Unit)\n",
    "2. SELU (Scaled exponential Linear Unit)\n",
    "\n",
    "A]. Leaky ReLU: max(0.01z, z) ; (z<0, z>=0)\n",
    "B]. Parametric ReLU: max(ɑz, z) ; (z<0, z>=0), ɑ is a training parameter.\n",
    "Pros:\n",
    "\t1. Non-saturated, unbounded\n",
    "\t2. Easily computed\n",
    "\t3. No dying ReLU Problem\n",
    "\t4. Close to 0 centred (Both +ve and -ve value)\n",
    "\n",
    "C] ELU(x) = [x, ɑ(e^x - 1)] ; (x>=0 , x<0)\n",
    "Pros:\n",
    "\t1. Non-saturated, unbounded\n",
    "\t2. Continuous, differentiable if ɑ=1.\n",
    "\t3. No dying ReLU Problem\n",
    "\t4. Close to 0 centred (Both +ve and -ve value)\n",
    "\n",
    "Cons:\n",
    "\t1. Computationally expensive\n",
    "\n",
    "D] SELU(x) = ʎ[x, ɑ(e^x - 1)] ; (x>=0 , x<0)\n",
    "ʎ ≈ 1.05, ɑ ≈ 1.6732632423543772848170429916717\n",
    "Pros:\n",
    "\t1. Non-saturated, unbounded\n",
    "\t2. Continuous, differentiable if ɑ=1.\n",
    "\t3. No dying ReLU Problem\n",
    "\t4. Close to 0 centred (Both +ve and -ve value)\n",
    "\t5. Self-normalizing //The best advantage\n",
    "\n",
    "Cons:\n",
    "\t1. Computationally expensive.\n",
    "\t2. Relatively newer, less adopted.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Weight Intialization:\n",
    "Problems:\n",
    "\t1. Vanishing Gradient\n",
    "\t2. Exploding Gradient\n",
    "\t3. Slow Convergence\n",
    "\n",
    "Why weights should not be zero intially?\n",
    "ReLU, tanh\n",
    "1. Training will not take place, as first output will be zero and so the derivatives.\n",
    "Sigmoid:\n",
    "1. All neurons in a single layer behaves as same neuron, so it acts as single perceptron.\n",
    ".\n",
    "Why weights should not be same non-zero values?\n",
    "1. All neurons of a layer will act like same neurons, so non-linear relations will be captured.\n",
    "\n",
    "Problems in random intialization, with small weights or large weights?\n",
    "We would set weights by random.randn()*0.01, leading to Vanishing Gradient Problem\n",
    "Intensity: tanh(Extreme problem) > Sigmoid(Still a significant problem) > ReLU(Very slow convergence)\n",
    "In large weights, it cause saturation problem in tanh and sigmoid.\n",
    "\n",
    "Heurisitics(Jugaad)\n",
    "Practical Solution:\n",
    "Xavier-Glorat Intialization: Normal, Uniform //Preferred for tanh\n",
    "He Init Intialization: Normal, Unifom //Preferred for ReLU\n",
    "\n",
    "A\\ Xavier Normal\n",
    "\tnp.random.randn(m,n) * ɑ\n",
    "\tVariance should be 1/n , n be the number of inputs to a particular node.\n",
    "\tɑ = standard deviation, √(1/n)\n",
    "\tIn techincal terms, np.random.randn(m,n) * √(1/fan_in),\n",
    "\tor some also write rarely, np.random.randn(m,n) * √[2/(fan_in+fan_out)]\n",
    "\n",
    "B] He Normal:\n",
    "\tHere, basically the factor becomes, √(2/fan_in)\n",
    "\n",
    "C] Xavier Uniform Distribution:\n",
    "Take the factors uniformly from:\n",
    "[-limit, limit], where limit stands for, limit = √[6/(fan_in+fan_out)]\n",
    "\n",
    "D] He Uniform Distribution:\n",
    "Take the factors uniformly from:\n",
    "[-limit, limit], where limit stands for, limit = √[6/(fan_in)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Put a hyperparameter, in layers, e.g. Dense(kernel_intializer = 'he-normal')\n",
    "Default is  'glorat-uniform',\n",
    "Uniform is better for shallow and wider networks.\n",
    "While, normal is better for deep networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Batch Normalization: It is an algorithmic method which makes the training of DNN faster and in more stable manner.\n",
    "It consists of normalizing activation vectors from hidden layers using the mean and variance of the current batch. This normalization step is applied right before (or right after) the non-linear function.\n",
    "Covariate Shift: \n",
    "It means you train the model for a particular use, but by data which is aligned to a particular unwanted features, lacking generalization of that question, which leads to bad results on test data.\n",
    "Like, training to identify rose and in X_train, all red colored roses are there but in X_test, you give yellow, white, all colored-roses.\n",
    "\n",
    "//Actually, internal covariate shift has no direct relationship between them, but is so named by authors of batch normalization for analogy in input distribution, there in datasets and here in inside network.\n",
    "Internal Covariate Shift:\n",
    "Due to changing input distribution, it is difficult for deep network layers to figure out output, it's like you are not fixed which exam you are going to give and you want to achieve AIR-1.\n",
    "\n",
    "So, we apply batch normalization to ensure that atleast some features of data would be same.\n",
    "Most popular is normalization before putting in activation function.\n",
    "Its done by \n",
    "z = [z-μ]/[σ + ∈],  ∈ is so that never denominator become zero when σ=0, usually it is negligible.\n",
    "µ: Mean of all points, σ is standard deviation.\n",
    "\n",
    "Now, after this normalization, z = Yz + B, where Y and B are learnable parameters, Y = 1 and B = 0 in Keras intially.\n",
    "And this Y and B are specific to each neuron.\n",
    "This is contradictory, as if Y = σ+∈ and B=µ, them this reverses and ultimately is of no use.\n",
    "But this provides flexibility to the neural network for training according to its own.\n",
    "\n",
    "See, in training we calculate mean and std of all the activation function inputs of the batch.\n",
    "Now, here EWMA, exponentially weighted moving average is calculated for further retaining of mean and std.\n",
    "Its calculated roughly by average of all means and std of all batches till date.\n",
    "So, in batch normalization, for each neuron, there are 2 learning parameters and 2 non-learning parameters.\n",
    "Non-learning in the sense, their gradient is not used in back-propagation, instead they are calculated on the go.\n",
    "\n",
    "Advantages:\n",
    "1. Stable: Hyperparameter tuning, wider range of values\n",
    "2. Faster: Higher learning rate is now acceptable, as problems like vanishing gradient and exploding gradient are now not an issue, as it normalizes data in each step.\n",
    "3. Regularization: As, the mean and std changes by EWMA, it causes noise, which itself causes regularization.\n",
    "4. Reduces weight intialization problems.\n",
    "\n",
    "To apply it use, between hidden layers.\n",
    "model.add(Dense())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense())\n",
    "\n",
    "μnew= ([1−α] * μold) + (α * μbatch)\n",
    "σnew= ([1−c] * σold) + (α * σbatch)\n",
    "\n",
    "where α is the momentum parameter.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizers:\n",
    "Actually deep learning is an optimisation technique used to minimize the loss between y_actual and y_pred.\n",
    "And so, we use the opimizer gradient descent with its 3 variants.\n",
    "But, it has problem of deciding learning rate and so some people try learning rate scheduler for it, but it also has haeavy dependence on data, and so poorly performs on test or new data.\n",
    "And, also for each parameters or weights we use same learning rate, which means for every direction, we are approaching with same speed, which is not fair.\n",
    "Local Minimum, is also a problem as sometimes it sticks to a particular minima, which is not global but best.\n",
    "Saddle Point Problem: Here, the value of gradients become very less, leading to slow convergence or stopping of training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Common Optimizers:\n",
    "1. Momentum\n",
    "2. Adagrad\n",
    "3. NAG\n",
    "4. RMSProp\n",
    "5. Adam\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizers:\n",
    "Exponentially Weighted Moving Average:\n",
    "Time Series Forecasting, Finance Forecasting, Signal Processing, Deep Learning Optimizers.\n",
    "Vnew= ([1−α] * Vnew) + (α * Vold)\n",
    "Here, α = 0.9 generally, and initially V1 = V0 or simply 0, but V0 is more correct.\n",
    "Here, 1/[1−α], is the number of days of which EWMA seems to be average.\n",
    "Therefore, less the α, more it sticks to the data.\n",
    "Now, by multiple substituting Vnew in Vnew1, subsequently in Vnew2, leads to α and 1-α factors, to old terms thus leading to their low weightage.\n",
    "It is inbuilt in pandas, df['meantemp'].ewm(alpha=0.9).mean()\n",
    "\n",
    "SGD with Momentum:\n",
    "\n",
    "Non-Convex Optimization: Problems are, \n",
    "High Curvature (Unstable)\n",
    "Consistent Gradient (Saddle)\n",
    "Noisy (Local minima)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wt+1 = Wt - ŋ∇Wt //Vanilla Gradient Descent\n",
    "Wt+1 = Wt - Vt //SGD with Momentum\n",
    "Vt =  ß*Vt-1 + ŋ∇Wt //Definition of Vt using EWMA\n",
    "0<ß<1, here ß is generally 0.9\n",
    "SGD with momentum takes longer steps.\n",
    "For analogy, if on any path, 4 people tell you single direction for your destination and in other case 3 people tell you one and one person tells another,\n",
    "then you will move forward faster in 1st case, while slowly in 2nd case.\n",
    "\n",
    "and another 1/1-ß, gives you number of days whose average you are considering.\n",
    "But, if ß=1, then it achives dynamic equilibrium and keeps oscillating forever.\n",
    "For, ß=0, its simply SGD only.\n",
    "\n",
    "Escapes local minimum due to its large updates due to its velocity.\n",
    "But, it oscillates due to its velocity even near global minimum. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Nesterov Accelerated Gradient(NAG):\n",
    "Wla = Wt - ß*Vt-1\n",
    "Vt = ß*Vt-1 + ŋ∇Wla\n",
    "Wt+1 = Wt - Vt\n",
    "la: look ahead\n",
    "\n",
    "Here, we first traverse with previous velocity and then calculate gradient on the new point and then traverse accordingly.\n",
    "It basically dampens the oscillation and thus reduce its epochs but again it may lead to local minimum, which was a fundamental problem sorted by momentum.\n",
    "\n",
    "Syntax:\n",
    "personalopt = tf.keras.optimizers.SGD(\n",
    "\tlearning_rate=0.01, momentum = 0.0, nesterov=False, name=\"SGD\", **kwargs\n",
    ")\n",
    "Put this in hyperparameter: optimizer in following way, optimizer=\"personalopt\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adaptive Gradient:\n",
    "1. Input Features have different scale.\n",
    "2. Features are sparse, meaning maximum values are zero.\n",
    "Actually, sparse data creates elongated bowl problem.\n",
    "\n",
    "In such situation,\n",
    "Consider a half cut cylinder, now with half face and full length, and a pit between the length.\n",
    "In Vanilla GD and momentum, if in one dimension there is steep slope it would go there first, then it starts moving in another parameters slope, it leads to longer path, if from corner of the cross section, it first reaches inner bottom, then slowly to mid.\n",
    "\n",
    "Now, in AdaGrad we keep different learning rate for different parameter, in such a fashion that if gradient is small, learning rate is bigger.\n",
    "Wt+1 = Wt - (ŋ∇Wt)/[√(Vt+∈)],  ∈ is just a small number so that denominator never becomes zero.\n",
    "Vt = Vt-1 + (∇Wt)^2, square so that it should not be negative and differentiable.\n",
    "Similarly for biases.\n",
    "\n",
    "AdaGrad: Is not meant for complex deep neural network, like non-convex optimization, as it never converges to global minima as gradually Vt becomes larger.\n",
    "Can be used for simple problems like linear regression, convex problems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RMSProp: Root Mean Square for Propagation\n",
    "Wt+1 = Wt - (ŋ∇Wt)/[√(Vt+∈)],  ∈ is just a small number so that denominator never becomes zero.\n",
    "Vt = ßVt-1 + (1-ß)[(∇Wt)^2],   ß is generally 0.95.\n",
    "So, here we are not giving that much weightage to old weights now.\n",
    "Disadvantage: Generally no disadvantages, a competitor for Adam.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adam: Adaptive Moment Estimation\n",
    "Most powerful used extensively in RNN, CNN, ANN.\n",
    "Ideas Used:\n",
    "1. Momentum\n",
    "2. Learning decay\n",
    "\n",
    "mt and Vt are called as moments.\n",
    "Wt+1 = Wt - {(ŋ∇Wt)/[√(Vt+∈)]}*mt,  ∈ is just a small number so that denominator never becomes zero.\n",
    "Where, mt = ß1mt-1 + (1-ß1)∇Wt : Momentum\n",
    "Vt = ß2Vt-1+ (1-ß2)(∇Wt)^2     : Learning Rate\n",
    "\n",
    "Bias Correction: t is epoch number, in denominator, not in numerator.\n",
    "mt^ = mt/(1-ß1t),\n",
    "Vt^ = Vt/(1-ß2t)\n",
    "ß1 = 0.9, ß2 = 0.99, Configurable as hyperparameter in keras.\n",
    "Bias Correction is meant for eliminating effects of first few steps where value of mt and Vt are zero, it does so by scaling mt and Vt at every step, according to the given expressions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For Hyperparameter tuning, use libraries like Keras Tuner\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

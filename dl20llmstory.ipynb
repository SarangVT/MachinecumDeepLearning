{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sequence to sequence architectures:\n",
    "Transformers, Encoder-Decoder, Attention.\n",
    "Unsupervised Learning:\n",
    "GAN (Generative Adversial Network)\n",
    "AutoEncoders\n",
    "\n",
    "In seq2seq there are asynchronous and synchronous, type of models.\n",
    "Synchronous: Gives number of outputs as the inputs.\n",
    "Asynchronous: Gives number of outputs different from the inputs.\n",
    "Applications:\n",
    "1. Question and Answering\n",
    "\n",
    "Encoder-Decoder->Attention->Transformer->Transfer Learning->LLMs\n",
    "\n",
    "In encoder-decoder architecture, the primary components are:\n",
    "1. Encoder: Takes the input and converts it into a context vector, generally they use LSTM or GRU, and if LSTM, it sends final hidden state and cell state to the decoder.\n",
    "2. Decoder: Takers the context vector they have a softmax layer at output, and they continuously in every timestamp give it as output to softmax layer, which then starts giving the output.\n",
    "It consists of a start as well as an end token, which marks when to start giving output as well as when to end.\n",
    "While training, we always feed the correct input in each timestamp to the LSTM instead of our output in previous timestamp, despite of our output coming wrong in intial timestamps, as it fastens the training process, but while predicting, we just feed the output as the next input in LSTM in decoder.\n",
    "\n",
    "We use embedding layer to convert the input into a dense vector, which is then fed to the encoder, as it helps reduce the dimensionality of the input and capture the context.\n",
    "We can use Deep LSTMs, iddeal for large paragraphs, as it have multiple context vectors, it can understand the hierarchial data.\n",
    "Reverse the input, it can improve the results, in specific languages where context is more in the intial words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Attention Mechanism:\n",
    "See, if we have to translate a large paragraph, we can't translate it in a single go, we have to take parts, but in encoder-decoder we are passing it in single context vector, it is not fair.\n",
    "So, we need something different that focuses on what is needed for that particular word from our data.\n",
    "\n",
    "We were using LSTMs, so we get intial input as context vector, and the final hidden state.\n",
    "So, in giving output earlier in LSTM in decoder we needed the previous hidden state, and the flowing context long-term vector, but in attention mechanism we need, an additional thing that is attention variable that captures the weighted attention of all words in the original input, needed for that translation.\n",
    "It is basically a weight which corresponds to every word of every encoder and decoder, so there are n*m weights, where n is timestamp size in encoder and m timestamp size in decoder.\n",
    "And, this weights are calculated on basis of hidden state of that particular decoder and hidden state of that particular encoder, which is then passed into an ANN(universal function approximator), to get the way in which hi, hj decides the weight ɑij, so in training we not only try to calculate ɑij, but the weights of the intermediate ANN also.\n",
    "And, so we get the output, originally when stated they used Bi-directional LSTMs and it was first mechanism to show steady improved performance over large paragraphs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Bahdanau Attention:\n",
    "Same as described earlier.\n",
    "In, it the weights for calculating ANN are updated at every timestamp, but they are same for every word.\n",
    "That's why the ANN is called as Time distributed FNN, (Fully Connected Layers), as it is updated with respect to time.\n",
    "Our ɑ is the ultimate output of ANN, softmax activation function and categorical_cross entropy loss function.\n",
    "Bahdanau Attention is also called as Additive Attention and the neural network is called as Alignment Model.\n",
    "\n",
    "Luong Attention:\n",
    "In it the change is ɑ is depending now on si, hj not on si-1 and hj, s stands for decoder timestamp, and h stands for encoder.\n",
    "It is done so that more updated information is feeded to the neural network.\n",
    "And, regrarding the neural network used earlier to find ɑ, it is now replaced by just dot product between si and hj, considering equal dimensions.\n",
    "As, ɑ just represents how much they are depended on each other, how much si requires hj, and so if vectors are near they give larger product and vice versa, so unnecessary computational complexity decreases.\n",
    "Experimentally, luong performed better than Bahdanau, also it was proposed later.\n",
    "Luong attention is called as multiplicative attention, as due to the dot product in si and hj.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transformers:\n",
    "It is made for seq2seq tasks, as they transform sequencial data to other sequential data, that's why transformers.\n",
    "Now, due to its flexibility, it was heavily research, and multi-modality capabilities arised and enhanced.\n",
    "Acceleration of Generative AI(The AI which generates text, image and other media).\n",
    "Pros:\n",
    "Scalable due to parallel input, transfer learning, multi-modal, integrated to other technologies like GANs, Reinforcement Learning.\n",
    "It can be fine tuned to our purpose.\n",
    "Cons:\n",
    "High computational complexity.\n",
    "Huge data else overfitting.\n",
    "Energy consumption.\n",
    "Interpretability, as it is blackbox model, as cause of results is not known due to Neural Network's feature, so in critical domains where cause of results is required so, there they are not adequate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First step in NLP is vectorization, converting words to numbers.\n",
    "Primitve techniques like bag of words which described semtences as number of times a word comes in it, for example sit sit eat is represented as [2 1 0 0 0], if 5 words are in vocabulary, but it doesn't carry semantic meaning and another one-hot encoding.\n",
    "Then, came embedding, which captured context, but it is also harmful as if majority sentences are pointing to a specific type for a word, so it would be biased towards that type, like if all words towards Apple is for fruit, so when it would be used for company it may show poor results.\n",
    "As, it is static embedding, means it is word-specific not context-specific, so word embedding should be changed according to the context.\n",
    "So, self-attention serves this purpose to convert static embedding to dynamic embedding or another static embedding, fine for that context.\n",
    "\n",
    "The context of a word depends on all words in the sentence.\n",
    "And the amount they depend is equal to their proximity in their vectorial forms.\n",
    "So, like we have sentence, I am Sarang.\n",
    "Then, e1, e2, e3 are corresponding embedding vectors of 'I', 'am', 'Sarang'.\n",
    "Then, e1 = w11e1 + w12e2 + w13e3.\n",
    "Where, w11 = norm(e1.e1T) and so on w12 = norm(e1.e2T), where this represents their dot-product, that's why transpose, as dot of 2 vectors shows how close they are, so it automatically infers their proximity.\n",
    "Then, before using e1 it is normalized using softmax, means exp(w11)/[exp(w11)+exp(w12)+exp(w13)]\n",
    "\n",
    "Now, advantage is parallel processing, as its just matrix multiplication and now it captures context of complete sentence, but it has cons as it doesn't consider sequence, thus wastes and looses sequential information.\n",
    "There are no parameters involved for training, so no training.\n",
    "But, it is not task-specific, so a phrase like piece of cake maybe translated to hindi as its lexical meaning, but we in our context is inferring it as easy work, so it is not adhering to its inner meaning.\n",
    "Here, the model is simply judging just on the basis of the sentence, and is not learning anything, so that it understands where to use what.\n",
    "\n",
    "Now, let's now in w12, we name, e1 as query, e2T as key and w12 as value.\n",
    "Now, its not fine that by the same vector you are finding match also, providing match also and you are assigning also.\n",
    "So, now we would find query, key and value for each embedding.\n",
    "As, for finding another matrix from a single matrix, is done by using multiplying it with another matrix, and so we would multiply it with Weight matrix whose weights would be trained in training period, and so would have knowledge of data on which it is trained.\n",
    "But, remember this 3 matrices W1, Wk, Wv would be same for each word to be processed and so everything can be processed parallely even now.\n",
    "We can bunch all the key vectors and multiply it with query, then put in softmax and then again multiplied with Wv. Which is then converted to embedding's shape.\n",
    "Means, eWq = Q, eWk = K, eWv = V, softmax(Q.KT) * V = Output.\n",
    "But, see empirically, dot products of low dimensional vectors possess low variance and of high dimension vectors possess high variance.\n",
    "So, its not absolutely true, but generally this is only true, as its like more space, more variety, more variance.\n",
    "And, more variance is a problem to activation functions like softmax, as if on a matrix softmax is applied, it due to its exponential nature, scales smaller values more smaller, and larger values more larger, eventually leading to stagnant training of small parameters and concentration on large parameters only.\n",
    "So, its like in a class only doubts of tall students raising hands are answered and overall learning of class depends on them only, dwarf people don' participate in overall learning of class.\n",
    "So, we cannot keep small embeddings for small variance, as by it we would not be able to keep various aspects of data, so ultimate goal is to convert high dimensional matrix to low variance.\n",
    "Now, see variance is approximately directly proportional to dimensionality of vector multiplying, like\n",
    "in, [1,3]*[3,1] and [1,3,2]*[3,1,2] the numbers are more in higher dimensions.\n",
    "Ans, as the variance, increases linearly and thus, standard deviation increases by square root of dimension, as we have to make standard deviation 1.\n",
    "\n",
    "Reason for nomenclature:\n",
    "Attention refers mathematical similarity to bahdanau and luong attention. Here, si(decoder) is query, hj(encoder) is key, and hj is only value, so due to its similar nature it is attention.\n",
    "Now, it is self as here we are not comparing two sequences, of encoder and decoder but comparing just their own sequence.\n",
    "So, that's why it is also called as intra-attention.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multi-Head Attention:\n",
    "Any particular sentence, may contain multiple meanings or perspectives, and we should capture them all in our ideal model in NLP, for example in Text Summarization we can summarize it from various perspectives.\n",
    "A famous sentence A man saw astronomer with a telescope. Can be inferred as a man possessing telescope, saw an astronomer while another perspective says a man saw astronomer who possessed telescope.\n",
    "We are neglecting a bit grammar here, just get the idea.\n",
    "In it we apply multiple self-attention, so due to multiple query, key and value ANN in between, we get multiple perspective from them, like in some there may be more connection in man and telescope while in another astronomer and telescope.\n",
    "In original paper, 8 self-attention were used.\n",
    "Architecture:\n",
    "We use multiple ANN to generate multiple embeddings, then we concatenate it and pass it through a ANN, say W0 weights and would resize it to the original size by linear transformation(matrix multiplication of W0 and concatenated embedding), and thus we get our embedding.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Positional Encoding:\n",
    "In self attention till now, was not able to capture the order of words, which is a fundamental weak point of self-attention compared to RNN or LSTM.\n",
    "And without this capability, it is not a proper NLP variant.\n",
    "So, what we require is the order, but we can't simply send the order number in a dimension, as the large numbers creates unstability in training for backpropagation.\n",
    "Even, if we normalize and send indexes, we would have different values for same index, if sentence has length 4, then 2nd place has 0.5, if length 5 then, 0.4, its confusing for the model to guess 2nd index.\n",
    "Also, discrete and unbounded data is not liked by neural network, and we are also not capturing relative positioning, (Distance is not calculated between different words).\n",
    "So, we want continuous, bounded, periodic function.\n",
    "So, we would make a positional encoding vector, using trignometric functions and size equal to size of embedding, like sin(x/2), sin(x/3), for unique representation of the number.\n",
    "P.E (pos, 2i) = sin(pos/ 10000^(2i/dmodel))\n",
    "P.E (pos, 2i+1) = cos(pos/ 1000^(2i/dmodel))\n",
    "Now, this vector is point-wise added to the original embedding which is then passed to self-attention.\n",
    "It is not concatenated to increase params and thus increasing training time directly by a factor of 2.\n",
    "Now, the later part in the positional embedding vector doesn't change for small data, but is useful when we use large text.\n",
    "Also, due to trigonometric functions, we can make a matrix which using sinAcosB + cosAsinB, type fformat means perfect combination of sin, cos, which would form perfect sin(A+B) type format leading to convert a vector at A, by multiplying a vector to become a vector at A+B.\n",
    "So, it has capability of relative positioning by perfect expressions of trignometry.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Layer Normalization:\n",
    "In batch normalization before passing the weighted sum to activation function, we normalize it separately, by subtracting the mean and dividing by standard deviation.\n",
    "The mean is calculated for every feature using batches of training data, and in prediction, a mean and a standard deviation is stored.\n",
    "But in sequential data with variable-sized data, if we have to apply batch normalization, we have to apply zero padding to short sentences, Then, if we calculate mean then there would be many zeroes, which would not allow fair representation of data.\n",
    "\n",
    "Now, in layer normalization we don't normalize for a feature, but we instead normalize for the layer or simply a sentence, so we instead normalize in row format instead of column, and each node has its Y, B for adjustment for weigthted, normalized sum just before activation function, as like batch normalization.\n",
    "So, now in layer normalization the zero for padding affects itself only. Remember zeroes still would be there for uniform dataset, but they would not affect other dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transformer architecture consists of primarily consists of encoder and decoder:\n",
    "Encoder Architecture of Transformer:\n",
    "A. Input Block:\n",
    "\t1. Tokenization of words.\n",
    "\t2. Vectorization: Words are converted to vector, embedding is used here.\n",
    "\t3. Positional Encoding\n",
    "B. Multi-Head Attention is applied on the input.\n",
    "C. Point-wise Addition by Residual Connection: Means the output of multi-head attention is added to the input of multi-head attention.\n",
    "D. Layer Normalization: We are normalizing vector of each word, to stabilize the training process.\n",
    "E. Feed Forward Block: 2 Layer NN(2048 neurons(ReLU), 512 neurons(linear)) + Input Layer, we introduced non-linearity by ReLU, and the shape is maintained after NN.\n",
    "F. Point-Wise Addition: Again input of Feed Forward NN and output of it are point-wise added.\n",
    "G. Layer Normalization\n",
    "H. The final output serves as the input for next encoder.\n",
    "\n",
    "Caution: The parameter values of all encoders are different and are calculated using training, just their architecture is same not the parameters.\n",
    "\n",
    "Residual Connections: \n",
    "Maybe due to stable training, for avoiding vanishing gradient in deep NN.\n",
    "Original data is preserved and still given value.\n",
    "Safety, if multi-head attention moves away from main point, it restricts it.\n",
    "\n",
    "Feed Forward Neural Network:\n",
    "Introducing Non-linearity and Learning more from data.\n",
    "\n",
    "Multiple Encoder Blocks:\n",
    "To gather the complexity of input natural language.\n",
    "Empirically 6 gave the researchers, best results for their tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Masked Self-Attention:\\nAutoregressive models: Means output of new data point requires data of previously generated data points.\\nLike, encoder-decoder model, where the LSTM requires the hidden state of previous timestamp along with context vector.\\nTransformer decoder is autoregressive at inference time and non-autoregressive at training time, because of masked self-attention.\\nMeans, we use teacher forcing in training, and we use correct dataset to serve as input(in self attention word embedding of any word depends on embedding of all other words used in sentence also) while training, so it allows for parallel processing.\\nBut, while predicting, we will not be available with the future tokens, as it is like data leakage.\\nBut, we want to preserve the parallel nature and so, we would add mask matrix in decoder to the original matrix.\\nLike, if we are training, and output is\\nI am reading.\\nI->[0 0 1] ; am-> [0 1 0] ; reading->[1 0 0]\\n[\\n[0 0 1] [0 1 0] [1 0 0]\\t\\t[0 0 0] [-inf -inf -inf] [-inf -inf -inf] \\n[0 0 1] [0 1 0] [1 0 0] + \\t[0 0 0] [0      0     0] [-inf -inf -inf] \\n[0 0 1] [0 1 0] [1 0 0]\\t\\t[0 0 0] [0      0     0] [0      0     0] \\n]\\nAnd as, softmax of -inf is 0, so while training we would preserve the original nature as well as model the original situation in word generation also, as without writing previous word we don't know the next word.\\nThus, we achieved non-autoregressive model during training, modelling real-world scenario.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Masked Self-Attention:\n",
    "Autoregressive models: Means output of new data point requires data of previously generated data points.\n",
    "Like, encoder-decoder model, where the LSTM requires the hidden state of previous timestamp along with context vector.\n",
    "Transformer decoder is autoregressive at inference time and non-autoregressive at training time, because of masked self-attention.\n",
    "Means, we use teacher forcing in training, and we use correct dataset to serve as input(in self attention word embedding of any word depends on embedding of all other words used in sentence also) while training, so it allows for parallel processing.\n",
    "But, while predicting, we will not be available with the future tokens, as it is like data leakage.\n",
    "But, we want to preserve the parallel nature and so, we would add mask matrix in decoder to the original matrix.\n",
    "Like, if we are training, and output is\n",
    "I am reading.\n",
    "I->[0 0 1] ; am-> [0 1 0] ; reading->[1 0 0]\n",
    "[\n",
    "[0 0 1] [0 1 0] [1 0 0]\t\t[0 0 0] [-inf -inf -inf] [-inf -inf -inf] \n",
    "[0 0 1] [0 1 0] [1 0 0] + \t[0 0 0] [0      0     0] [-inf -inf -inf] \n",
    "[0 0 1] [0 1 0] [1 0 0]\t\t[0 0 0] [0      0     0] [0      0     0] \n",
    "]\n",
    "And as, softmax of -inf is 0, so while training we would preserve the original nature as well as model the original situation in word generation also, as without writing previous word we don't know the next word.\n",
    "Thus, we achieved non-autoregressive model during training, modelling real-world scenario.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cross Attention, when all self-attention in a multi-head attention are not from the same source.\\nAs, we should gain information from what we have written till now and also, our overall context by encoder.\\nSo, query vector in that attention mechanism corresponds to the output sequence by decoder and the key and value vector corresponds to the input sequence by encoder.\\nFor each embedding(means each word), we get corresponding input vector from encoder, but the number of output vector remains same and internal params also, as just query vector is formed by output from decoder.\\nSo, it is actually impacting each self-attention internally in multi-head attention, its not that one self-attention is different.\\nThe output number of embedding is equal to the output of decoder.\\nBut, in writing expression they are function of input words from encoder only, difference is how they are depending internally changed, as query came from output of decoder.\\nIts very similar to bahdanau attention.\\nApplications:\\n1. Multi-modal\\n2. Seq2Seq\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Cross Attention, when all self-attention in a multi-head attention are not from the same source.\n",
    "As, we should gain information from what we have written till now and also, our overall context by encoder.\n",
    "So, query vector in that attention mechanism corresponds to the output sequence by decoder and the key and value vector corresponds to the input sequence by encoder.\n",
    "For each embedding(means each word), we get corresponding input vector from encoder, but the number of output vector remains same and internal params also, as just query vector is formed by output from decoder.\n",
    "So, it is actually impacting each self-attention internally in multi-head attention, its not that one self-attention is different.\n",
    "The output number of embedding is equal to the output of decoder.\n",
    "But, in writing expression they are function of input words from encoder only, difference is how they are depending internally changed, as query came from output of decoder.\n",
    "Its very similar to bahdanau attention.\n",
    "Applications:\n",
    "1. Multi-modal\n",
    "2. Seq2Seq\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Decoder Training Architecture:\\nOutputs refers to answer of that question.\\n1. Shifting outputs right to attach start flag.\\n2. Tokenizer\\n3. Embedding\\n4. Positional Encoding, adding pointwise the position vectors.\\n5. Masked Multi-Head Attention\\n6. Residual connections, adding the outputs directly to the outputs from masked multi-head attention.\\n7. Layer Normalization\\n8. Cross Attention: Final Encoder\\'s (Key, Value) + Output\\'s(Query)\\n9. Residial Connections, adding our decoder(not encoder) input to multi-head attention with the final output of multi-head attention.\\n10. Layer Normalization\\n11. Feed Forward Neural Network (2048 neurons(ReLU), 512 neurons(Linear)), we would pass batch wise by forming collective matrix, at last dimensions remains intact.\\n12. Residual Connection: Input and output of Feed Forward NN is point-wise added.\\n13. Layer Normalization\\n14. Output Probabilities of Decoder Block-1\\n15. Process continues till last block (original paper contained 6 decoder blocks)\\n16. Linear Layer, contains v nodes, v is equal to vocabulary of output words, activation function is linear, input given as matrix of all output words, means word\\'s vectors stacked over each other.\\n17. Softmax Layer, for converting the output into probability, the word with highest probability would be the answer.\\n18. Get End Flag and give stop giving output.\\n\\nAs, input to softmax activation function contains all output vocabulary, so word with highest probability in the v-sized vector outputed by softmax would be our final unqiue output.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"Decoder Training Architecture:\n",
    "Outputs refers to answer of that question.\n",
    "1. Shifting outputs right to attach start flag.\n",
    "2. Tokenizer\n",
    "3. Embedding\n",
    "4. Positional Encoding, adding pointwise the position vectors.\n",
    "5. Masked Multi-Head Attention\n",
    "6. Residual connections, adding the outputs directly to the outputs from masked multi-head attention.\n",
    "7. Layer Normalization\n",
    "8. Cross Attention: Final Encoder's (Key, Value) + Output's(Query)\n",
    "9. Residial Connections, adding our decoder(not encoder) input to multi-head attention with the final output of multi-head attention.\n",
    "10. Layer Normalization\n",
    "11. Feed Forward Neural Network (2048 neurons(ReLU), 512 neurons(Linear)), we would pass batch wise by forming collective matrix, at last dimensions remains intact.\n",
    "12. Residual Connection: Input and output of Feed Forward NN is point-wise added.\n",
    "13. Layer Normalization\n",
    "14. Output Probabilities of Decoder Block-1\n",
    "15. Process continues till last block (original paper contained 6 decoder blocks)\n",
    "16. Linear Layer, contains v nodes, v is equal to vocabulary of output words, activation function is linear, input given as matrix of all output words, means word's vectors stacked over each other.\n",
    "17. Softmax Layer, for converting the output into probability, the word with highest probability would be the answer.\n",
    "18. Get End Flag and give stop giving output.\n",
    "\n",
    "As, input to softmax activation function contains all output vocabulary, so word with highest probability in the v-sized vector outputed by softmax would be our final unqiue output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Decoder Inference Architecture:\\nIt was behaving non-autoregressive manner in training time, but in inference time works in autoregressive manner.\\nThe only difference is in the output embeddings served as input to decoder.\\nFirst time, <sos>, start of sentence is served as intial output and then, the word which is currently generated is served as next input to successive decoder, till last steps, remember no tokens are concatenated as it breaks the rule that at every decoder only, one word or token is generated, as if we take the previous token its like changing the word earlier predicted as it states the word next to it, but it needs to be calculated for passing in future decoder.\\nEven at inference masked multi-head attention is done, so in individual self-attention to <sos> no further generated words are shown, as the treatment given in training is replicated for ideal output.\\nFinally, we get end flag and we stop the generation.\\nSo, in training we give output as whole and get output as whole, but at prediction time, the tokens gradually increases.\\nSo, as decoders generate one word at a time, LLMs use hundreds of decoders and encoder or decoder only models.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Decoder Inference Architecture:\n",
    "It was behaving non-autoregressive manner in training time, but in inference time works in autoregressive manner.\n",
    "The only difference is in the output embeddings served as input to decoder.\n",
    "First time, <sos>, start of sentence is served as intial output and then, the word which is currently generated is served as next input to successive decoder, till last steps, remember no tokens are concatenated as it breaks the rule that at every decoder only, one word or token is generated, as if we take the previous token its like changing the word earlier predicted as it states the word next to it, but it needs to be calculated for passing in future decoder.\n",
    "Even at inference masked multi-head attention is done, so in individual self-attention to <sos> no further generated words are shown, as the treatment given in training is replicated for ideal output.\n",
    "Finally, we get end flag and we stop the generation.\n",
    "So, in training we give output as whole and get output as whole, but at prediction time, the tokens gradually increases.\n",
    "So, as decoders generate one word at a time, LLMs use hundreds of decoders and encoder or decoder only models.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
